ğŸ§­ ç«¯åˆ°ç«¯éªŒè¯ï¼ˆDocker Postgres + Redis + æœ¬æœåŠ¡ï¼‰

ä»¥ä¸‹æ­¥éª¤æ¼”ç¤ºä» Alertmanager Webhook åˆ°æ•°æ®åº“è½åº“çš„å®Œæ•´é“¾è·¯éªŒè¯ï¼š

1) å¯åŠ¨ Postgresï¼ˆDockerï¼‰

```bash
docker run --name zeroops-pg \
  -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=postgres -e POSTGRES_DB=zeroops \
  -p 5432:5432 -d postgres:16
```

1b) å¯åŠ¨ Redisï¼ˆDockerï¼‰

```bash
docker run --name zeroops-redis -p 6379:6379 -d redis:7-alpine
```

2) åˆå§‹åŒ–å‘Šè­¦ç›¸å…³è¡¨
è¿è¡Œé›†æˆæµ‹è¯•ï¼ˆéœ€ Postgres å®ä¾‹ä¸ `-tags=integration`ï¼‰å¯éªŒè¯æ’å…¥æˆåŠŸï¼š
```bash
go test ./internal/alerting/service/receiver -tags=integration -run TestPgDAO_InsertAlertIssue -v
```

3) é…ç½®ç¯å¢ƒå˜é‡å¹¶å¯åŠ¨æœåŠ¡ï¼ˆå¦å¼€ä¸€ä¸ª shell åå°è¿è¡Œï¼‰

```bash
export DB_HOST=localhost DB_PORT=5432 DB_USER=postgres DB_PASSWORD=postgres DB_NAME=zeroops DB_SSLMODE=disable
export ALERT_WEBHOOK_BASIC_USER=alert ALERT_WEBHOOK_BASIC_PASS=REDACTED
export REDIS_ADDR=localhost:6379 REDIS_PASSWORD="" REDIS_DB=0
nohup go run ./cmd/zeroops -- 1>/tmp/zeroops.out 2>&1 &
```

4) ç”¨ curl æ¨¡æ‹Ÿ Alertmanager å‘é€ firing äº‹ä»¶

```bash
curl -u alert:REDACTED -H 'Content-Type: application/json' \
  -X POST http://localhost:8080/v1/integrations/alertmanager/webhook -d '{
  "receiver":"our-webhook",
  "status":"firing",
  "alerts":[{
    "status":"firing",
    "labels":{"alertname":"HighRequestLatency","service":"serviceA","severity":"P1","idc":"yzh"},
    "annotations":{"summary":"p95 latency over threshold","description":"apitime p95 > 450ms"},
    "startsAt":"2025-05-05T11:00:00Z",
    "endsAt":"0001-01-01T00:00:00Z",
    "generatorURL":"http://prometheus/graph?g0.expr=...",
    "fingerprint":"3b1b7f4e8f0e"
  }],
  "groupLabels":{"alertname":"HighRequestLatency"},
  "commonLabels":{"service":"serviceA","severity":"P1"},
  "version":"4"
}'
```

5) åœ¨æ•°æ®åº“ä¸­éªŒè¯ï¼ˆåº”çœ‹åˆ°ä¸€è¡Œ Open/P1/Pending ä¸”æ ‡é¢˜åŒ¹é…çš„è®°å½•ï¼‰

```bash
docker exec -i zeroops-pg psql -U postgres -d zeroops -c \
  "SELECT id,state,level,alert_state,title,alert_since FROM alert_issues WHERE title='p95 latency over threshold' AND alert_since='2025-05-05 11:00:00'::timestamp;"
```
```bash
# æ›´æ˜“è¯»ï¼ˆæ ¼å¼åŒ– JSONï¼‰labels
docker exec -i zeroops-pg psql -U postgres -d zeroops -c \
   "SELECT jsonb_pretty(labels::jsonb) AS label FROM alert_issues WHERE title='p95 latency over threshold' AND alert_since='2025-05-05 11:00:00'::timestamp;"

```

6)ï¼ˆå¯é€‰ï¼‰è¿è¡Œå¸¦é›†æˆæ ‡ç­¾çš„æœ€å° DAO æµ‹è¯•

```bash
DB_HOST=localhost DB_PORT=5432 DB_USER=postgres DB_PASSWORD=postgres DB_NAME=zeroops DB_SSLMODE=disable \
go test ./internal/alerting/service/receiver -tags=integration -run TestPgDAO_InsertAlertIssue -v
```


receiver/ â€” ä» Alertmanager Webhook åˆ° alert_issues å…¥åº“çš„å®æ–½è®¡åˆ’

ç›®æ ‡ï¼šå½“ Alertmanager å‘æœ¬æœåŠ¡å‘èµ· POST JSON æ—¶ï¼Œç¬¬ä¸€æ¬¡åˆ›å»ºå‘Šè­¦è®°å½•å¹¶è½è¡¨ alert_issuesï¼Œå­—æ®µè§„åˆ™ï¼š
	â€¢	state é»˜è®¤ Open
	â€¢	alertState é»˜è®¤ Pending
	â€¢	å…¶ä½™å­—æ®µæŒ‰ webhook è¯·æ±‚ä½“è§£æã€æ ¡éªŒåå†™å…¥

æœ¬è®¡åˆ’ä»…è¦†ç›–ã€Œé¦–æ¬¡åˆ›å»ºã€é€»è¾‘ï¼›resolvedï¼ˆæ¢å¤ï¼‰æ›´æ–°é€»è¾‘å¯åœ¨åç»­è¡¥å……ï¼ˆä¾‹å¦‚åˆ‡æ¢ state=Closedã€alertState=Restoredï¼‰ã€‚

â¸»

â‘  ç›®å½•ä¸æ–‡ä»¶å‡†å¤‡

åœ¨ alerting/service/receiver/ ä¸‹æ–°å»ºå¦‚ä¸‹æ–‡ä»¶ï¼ˆæŒ‰æ¨¡å—èŒè´£åˆ’åˆ†ï¼‰ï¼š

alerting/
â””â”€ service/
   â””â”€ receiver/
      â”œâ”€ README.md                 # â† å°±æ”¾æœ¬æ–‡æ¡£
      â”œâ”€ router.go                 # æ³¨å†Œè·¯ç”±ï¼šPOST /v1/integrations/alertmanager/webhook
      â”œâ”€ handler.go                # HTTP å…¥å£ï¼Œæ¥æ”¶ä¸æ•´ä½“ç¼–æ’
      â”œâ”€ dto.go                    # å…¥å‚ï¼ˆAlertmanager Webhookï¼‰ä¸å†…éƒ¨ DTO å®šä¹‰
      â”œâ”€ validator.go              # å­—æ®µæ ¡éªŒï¼ˆå¿…å¡«/æšä¸¾/æ—¶é—´æ ¼å¼ç­‰ï¼‰
      â”œâ”€ mapper.go                 # æ˜ å°„ï¼šAM payload â†’ alert_issues è¡Œè®°å½•
      â”œâ”€ dao.go                    # DB è®¿é—®ï¼ˆInsert/Query/äº‹åŠ¡/é‡è¯•ï¼‰
      â”œâ”€ cache.go                  # Redis å®¢æˆ·ç«¯ä¸å†™é€šç¼“å­˜ï¼ˆWrite-throughï¼‰
      â”œâ”€ idempotency.go            # å¹‚ç­‰é”®ç”Ÿæˆä¸â€œå·²å¤„ç†â€å¿«é€Ÿåˆ¤æ–­ï¼ˆåº”ç”¨å±‚ï¼‰
      â””â”€ errors.go                 # ç»Ÿä¸€é”™è¯¯å®šä¹‰ï¼ˆå‚æ•°é”™è¯¯/DBé”™è¯¯ç­‰ï¼‰

è‹¥ä½ çš„ DB è¿æ¥å°è£…åœ¨ alerting/database/ï¼Œdao.go é‡Œç›´æ¥å¼•å…¥å…¬ç”¨çš„ db å®¢æˆ·ç«¯å³å¯ã€‚

â¸»

â‘¡ è·¯ç”±ä¸å…¥å£

router.go

// package receiver
func RegisterReceiverRoutes(r *gin.Engine, h *Handler) {
    r.POST("/v1/integrations/alertmanager/webhook", h.AlertmanagerWebhook)
}

handler.go

type Handler struct {
    dao *DAO
    cache *Cache // Redis å†™é€š
}

func NewHandler(dao *DAO, cache *Cache) *Handler { return &Handler{dao: dao, cache: cache} }

func (h *Handler) AlertmanagerWebhook(c *gin.Context) {
    var req AMWebhook // dto.go ä¸­å®šä¹‰çš„ Alertmanager è¯·æ±‚ä½“ç»“æ„
    if err := c.BindJSON(&req); err != nil {
        c.JSON(http.StatusBadRequest, gin.H{"ok": false, "error": "invalid JSON"})
        return
    }

    // 1) åŸºæœ¬å­—æ®µæ ¡éªŒ
    if err := ValidateAMWebhook(&req); err != nil {
        c.JSON(http.StatusBadRequest, gin.H{"ok": false, "error": err.Error()})
        return
    }

    // 2) ä»…å¤„ç† status == "firing" çš„é¦–æ¬¡åˆ›å»º
    if strings.ToLower(req.Status) != "firing" {
        c.JSON(http.StatusOK, gin.H{"ok": true, "msg": "ignored (not firing)"})
        return
    }

    // 3) å¯¹æ¯æ¡ alert åšè½åº“ï¼ˆå¯èƒ½ä¸€æ‰¹å¤šä¸ªï¼‰
    //    å¹‚ç­‰é”®å»ºè®®ï¼šfingerprint + startsAtï¼ˆåŒä¸€å‘Šè­¦èµ·å§‹æ—¶é—´è§†ä¸ºåŒä¸€äº‹ä»¶ï¼‰
    created := 0
    for _, a := range req.Alerts {
        key := BuildIdempotencyKey(a)         // idempotency.go
        if AlreadySeen(key) {                 // åº”ç”¨å±‚çŸ­è·¯ï¼ˆå¯é€‰ï¼‰
            continue
        }

        row, mapErr := MapToAlertIssueRow(&req, &a) // mapper.go â†’ ç»„è£… alert_issues è¡Œ
        if mapErr != nil {
            // å•æ¡å¤±è´¥ä¸å½±å“å…¶å®ƒï¼Œè®°å½•æ—¥å¿—å³å¯
            continue
        }

        // 4) æ’å…¥ DBï¼ˆç¬¬ä¸€æ¬¡åˆ›å»ºå¼ºåˆ¶ state=Open, alertState=Pendingï¼‰
        if err := h.dao.InsertAlertIssue(c, row); err != nil {
            // è‹¥å”¯ä¸€çº¦æŸå†²çª/ç½‘ç»œæŠ–åŠ¨ç­‰ï¼Œè®°å½•åç»§ç»­
            continue
        }
        // 5) åŒæ­¥å†™å…¥ service_statesï¼ˆhealth_state=Errorï¼›detail/resolved_at/correlation_id ç•™ç©ºï¼‰
        //    service ä» labels.service å–ï¼›version å¯ä» labels.service_version å–ï¼ˆå¯ç©ºï¼‰
        if err := h.dao.UpsertServiceState(c, a.Labels["service"], a.Labels["service_version"], row.AlertSince, "Error"); err != nil {
            // ä»…è®°å½•é”™è¯¯ï¼Œä¸é˜»æ–­ä¸»æµç¨‹
        }
        // 6) å†™é€šåˆ° Redisï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼Œå¤±è´¥ä»…è®°å½•æ—¥å¿—ï¼‰
        //    alert_issues
        if err := h.cache.WriteIssue(c, row, a); err != nil {
            // ä»…è®°å½•é”™è¯¯ï¼Œé¿å…å½±å“ Alertmanager é‡è¯•é€»è¾‘
        }
        //    service_states
        _ = h.cache.WriteServiceState(c, a.Labels["service"], a.Labels["service_version"], row.AlertSince, "Error")
        MarkSeen(key) // è®°å¿†å¹‚ç­‰é”®
        created++
    }

    c.JSON(http.StatusOK, gin.H{"ok": true, "created": created})
}


â¸»

â‘¢ å…¥å‚ DTO ä¸å†…éƒ¨ç»“æ„

dto.goï¼ˆAlertmanager Webhook è½½è· + å†…éƒ¨æ’å…¥ç»“æ„ï¼‰

type KV map[string]string

// æ¥è‡ª Alertmanager çš„å•æ¡å‘Šè­¦
type AMAlert struct {
    Status       string    `json:"status"`       // firing|resolved
    Labels       KV        `json:"labels"`       // åŒ…å« alertnameã€serviceã€severity ç­‰
    Annotations  KV        `json:"annotations"`  // åŒ…å« summary/description ç­‰
    StartsAt     time.Time `json:"startsAt"`
    EndsAt       time.Time `json:"endsAt"`
    GeneratorURL string    `json:"generatorURL"`
    Fingerprint  string    `json:"fingerprint"`  // ç”¨äºå¹‚ç­‰
}

// Webhook æ ¹å¯¹è±¡
type AMWebhook struct {
    Receiver          string    `json:"receiver"`
    Status            string    `json:"status"`            // firing|resolved
    Alerts            []AMAlert `json:"alerts"`
    GroupLabels       KV        `json:"groupLabels"`
    CommonLabels      KV        `json:"commonLabels"`
    CommonAnnotations KV        `json:"commonAnnotations"`
    ExternalURL       string    `json:"externalURL"`
    Version           string    `json:"version"`
    GroupKey          string    `json:"groupKey"`
}

// å‡†å¤‡æ’å…¥ alert_issues çš„è¡Œï¼ˆä¸è¡¨å­—æ®µä¸€ä¸€å¯¹åº”ï¼‰
type AlertIssueRow struct {
    ID         string          // uuid
    State      string          // enum: Open/Closed ï¼ˆé¦–æ¬¡å›ºå®š Openï¼‰
    Level      string          // varchar(32): P0/P1/P2/Warning
    AlertState string          // enum: Pending/InProcessing/Restored/AutoRestoredï¼ˆé¦–æ¬¡å›ºå®š Pendingï¼‰
    Title      string          // varchar(255)
    LabelJSON  json.RawMessage // json: æ ‡å‡†åŒ–åçš„ [{key,value}]
    AlertSince time.Time       // timestamp: ç”¨ StartsAt
}


â¸»

â‘£ å­—æ®µæ ¡éªŒï¼ˆvalidatorï¼‰

validator.go

func ValidateAMWebhook(w *AMWebhook) error {
    if w == nil { return errors.New("nil payload") }
    if len(w.Alerts) == 0 { return errors.New("alerts empty") }
    // å¯åŠ å¤§å°é™åˆ¶ï¼šlen(alerts) <= Nï¼›é˜²å·¨é‡ payload
    for i := range w.Alerts {
        a := &w.Alerts[i]
        if a.StartsAt.IsZero() { return fmt.Errorf("alerts[%d].startsAt empty", i) }
        // å…è®¸ç©º annotations.summaryï¼Œä½†åç»­ä¼šç”¨å›é€€è§„åˆ™ç”Ÿæˆ title
        if a.Status == "" { a.Status = "firing" } // å®¹é”™
    }
    return nil
}

var allowedLevels = map[string]bool{"P0":true,"P1":true,"P2":true,"Warning":true}

func NormalizeLevel(sev string) string {
    s := strings.ToUpper(strings.TrimSpace(sev))
    if allowedLevels[s] { return s }
    // è‹¥ä¸ºç©º/ä¸åˆæ³•ï¼Œå¯è®¾ç½®é»˜è®¤æˆ–äº¤ç»™ severity æ¨¡å—å†è¯„ä¼°
    return "Warning"
}


â¸»

â‘¤ æ˜ å°„è§„åˆ™ï¼ˆmapperï¼‰

ç›®æ ‡ï¼šå°† Alertmanager çš„å•æ¡ AMAlert â†’ AlertIssueRowã€‚
	â€¢	idï¼šuuid.NewString()
	â€¢	stateï¼šOpenï¼ˆé¦–æ¬¡åˆ›å»ºå¼ºåˆ¶ï¼‰
	â€¢	alertStateï¼šInProcessingï¼ˆé¦–æ¬¡åˆ›å»ºå¼ºåˆ¶ï¼‰
	â€¢	levelï¼šNormalizeLevel(alert.Labels["severity"])
	â€¢	titleï¼šä¼˜å…ˆ annotations.summaryï¼Œå¦åˆ™æ‹¼ï¼š{idc} {service} {alertname} ...
	â€¢	labelï¼šæŠŠ labels å±•å¹³æˆ [{key,value}]ï¼ˆé¢å¤–åŠ ä¸Šä¸€äº›å…³é”®æ¥æºä¿¡æ¯ï¼šam_fingerprintã€generatorURLã€groupKeyï¼‰
	â€¢	alertSinceï¼šStartsAtï¼ˆç»Ÿä¸€è½¬ UTCï¼‰

mapper.go

func MapToAlertIssueRow(w *AMWebhook, a *AMAlert) (*AlertIssueRow, error) {
    // 1) Title
    title := strings.TrimSpace(a.Annotations["summary"])
    if title == "" {
        // fallbackï¼šå°½é‡ä¿¡æ¯é‡å¤§ä¸”â‰¤255
        title = fmt.Sprintf("%s %s %s",
            a.Labels["idc"], a.Labels["service"], a.Labels["alertname"])
        title = strings.TrimSpace(title)
        if title == "" { title = "Alert from Alertmanager" }
    }
    if len(title) > 255 { title = title[:255] }

    // 2) Level
    level := NormalizeLevel(a.Labels["severity"])

    // 3) Labels â†’ []{key,value}
    //    é™„åŠ æŒ‡çº¹ç­‰æ–¹ä¾¿åç»­æŸ¥è¯¢/å¯¹è´¦
    flat := make([]map[string]string, 0, len(a.Labels)+3)
    for k, v := range a.Labels {
        flat = append(flat, map[string]string{"key": k, "value": v})
    }
    if a.Fingerprint != "" {
        flat = append(flat, map[string]string{"key": "am_fingerprint", "value": a.Fingerprint})
    }
    if g := strings.TrimSpace(a.GeneratorURL); g != "" {
        flat = append(flat, map[string]string{"key": "generatorURL", "value": g})
    }
    if w.GroupKey != "" {
        flat = append(flat, map[string]string{"key": "groupKey", "value": w.GroupKey})
    }
    b, _ := json.Marshal(flat)

    // 4) Row
    return &AlertIssueRow{
        ID:         uuid.NewString(),
        State:      "Open",
        AlertState: "Pending",
        Level:      level,
        Title:      title,
        LabelJSON:  b,
        AlertSince: a.StartsAt.UTC(), // å»ºè®®ç»Ÿä¸€ UTC
    }, nil
}


â¸»

â‘¥ å¹‚ç­‰ï¼ˆidempotencyï¼‰

è™½ç„¶æœ¬æ­¥éª¤ä¸»è¦æè¿°â€œé¦–æ¬¡åˆ›å»ºâ€ï¼Œä½†ä¸ºäº†é¿å…é‡å¤æ’å…¥ï¼Œå»ºè®®å¼•å…¥åº”ç”¨å±‚å¹‚ç­‰ï¼ˆæ— é¡»æ”¹è¡¨ç»“æ„ï¼‰ï¼š

idempotency.go

func BuildIdempotencyKey(a AMAlert) string {
    return a.Fingerprint + "|" + a.StartsAt.UTC().Format(time.RFC3339Nano)
}

// å¯ä»¥ç”¨å†…å­˜ LRU/Redisï¼›æˆ–å…¥åº“å‰å…ˆæŒ‰ (am_fingerprint + startsAt) æŸ¥è¯¢æ˜¯å¦å­˜åœ¨
func AlreadySeen(key string) bool { /* TODO */ return false }
func MarkSeen(key string)         { /* TODO */ }

è‹¥åç»­å…è®¸è°ƒæ•´è¡¨ç»“æ„ï¼Œå¯æŠŠ am_fingerprint å•åˆ—åŒ–å¹¶ä¸ alertSince ç»„æˆå”¯ä¸€ç´¢å¼•ï¼Œå¹‚ç­‰æ›´ç¨³ã€‚

â¸»

â‘¦ æ•°æ®è®¿é—®ï¼ˆDAOï¼‰

dao.goï¼ˆç¤ºä¾‹ä½¿ç”¨ pgx / database/sqlï¼Œé‡ç‚¹æ˜¯å‚æ•°åŒ–ä¸äº‹åŠ¡ï¼‰

type DAO struct{ DB *pgxpool.Pool }

func (d *DAO) InsertAlertIssue(ctx context.Context, r *AlertIssueRow) error {
    const q = `
    INSERT INTO alert_issues
        (id, state, level, alert_state, title, labels, alert_since)
    VALUES
        ($1, $2, $3, $4, $5, $6, $7)
    `
    _, err := d.DB.Exec(ctx, q,
        r.ID, r.State, r.Level, r.AlertState, r.Title, r.LabelJSON, r.AlertSince)
    return err
}

æ³¨æ„ï¼š
	â€¢	label åˆ—ç±»å‹ä¸º jsonï¼ˆå»ºè®®å®é™…ä½¿ç”¨ jsonbï¼‰ï¼Œæ­¤å¤„ç”¨ json.RawMessage å‚æ•°åŒ–å†™å…¥å³å¯ã€‚
	â€¢	ä½¿ç”¨ Exec/Prepare éƒ½å¯ï¼Œç¡®ä¿ä¸æ‹¼æ¥å­—ç¬¦ä¸²ï¼Œé˜²æ³¨å…¥ã€‚
	â€¢	ç”Ÿäº§å»ºè®®å¢åŠ ï¼šé‡è¯•ç­–ç•¥ã€æ’å…¥è€—æ—¶ç›‘æ§ã€é”™è¯¯åˆ†çº§ï¼ˆå”¯ä¸€å†²çª vs ç½‘ç»œæŠ–åŠ¨ï¼‰ã€‚

â¸»

â‘§ Redis ç¼“å­˜å†™é€šï¼ˆWrite-throughï¼‰ä¸åˆ†å¸ƒå¼å¹‚ç­‰

ç›®æ ‡ï¼šåœ¨æˆåŠŸå†™å…¥ PostgreSQL åï¼Œå°†å…³é”®æ•°æ®å†™å…¥ Redisï¼Œæ—¢ä¸ºå‰ç«¯æŸ¥è¯¢æä¾›åŠ é€Ÿç¼“å­˜ï¼Œä¹Ÿä¸ºåç»­å®šæ—¶ä»»åŠ¡æä¾›å¿«é€Ÿè¯»å–èƒ½åŠ›ï¼›åŒæ—¶ç”¨ Redis æä¾›è·¨å®ä¾‹å¹‚ç­‰æ§åˆ¶ã€‚

ä¾èµ–ï¼š

```bash
go get github.com/redis/go-redis/v9
```

é…ç½®ï¼ˆç¯å¢ƒå˜é‡ï¼‰ï¼š

```
REDIS_ADDR=localhost:6379
REDIS_PASSWORD=""
REDIS_DB=0
```

key è®¾è®¡ä¸ TTLï¼š

- alert:issue:{id} â†’ JSONï¼ˆAlertIssueRow + è¡¥å……å­—æ®µï¼‰ï¼ŒTTL 3d
- alert:idemp:{fingerprint}|{startsAtRFC3339Nano} â†’ "1"ï¼ŒTTL 10mï¼ˆç”¨äºåˆ†å¸ƒå¼å¹‚ç­‰ SETNXï¼‰
- alert:index:open â†’ Set(issues...)ï¼Œæ—  TTLï¼ˆæ¢å¤æ—¶å†ç§»é™¤ï¼‰
- alert:index:svc:{service}:open â†’ Set(issues...)ï¼Œæ—  TTL
// service_states ç¼“å­˜
- service_state:{service}:{version} â†’ JSONï¼ˆservice/version/report_at/health_stateï¼‰ï¼ŒTTL 3d
- service_state:index:service:{service} â†’ Set(keys)
- service_state:index:health:{health_state} â†’ Set(keys)

cache.goï¼ˆç¤ºä¾‹ï¼‰ï¼š

```go
type Cache struct{ R *redis.Client }

func NewCacheFromEnv() *Cache {
    db, _ := strconv.Atoi(os.Getenv("REDIS_DB"))
    c := redis.NewClient(&redis.Options{Addr: os.Getenv("REDIS_ADDR"), Password: os.Getenv("REDIS_PASSWORD"), DB: db})
    return &Cache{R: c}
}

// å†™é€šï¼šissue ä¸»é”®å¯¹è±¡ + ç´¢å¼•é›†åˆ
func (c *Cache) WriteIssue(ctx context.Context, r *AlertIssueRow, a AMAlert) error {
    if c == nil || c.R == nil { return nil }
    key := "alert:issue:" + r.ID
    payload := map[string]any{
        "id": r.ID, "state": r.State, "level": r.Level, "alertState": r.AlertState,
        "title": r.Title, "labels": json.RawMessage(r.LabelJSON), "alertSince": r.AlertSince,
        "fingerprint": a.Fingerprint, "service": a.Labels["service"], "alertname": a.Labels["alertname"],
    }
    b, _ := json.Marshal(payload)
    svc := strings.TrimSpace(a.Labels["service"])
    pipe := c.R.Pipeline()
    pipe.Set(ctx, key, b, 72*time.Hour)
    pipe.SAdd(ctx, "alert:index:open", r.ID)
    if svc != "" {
        pipe.SAdd(ctx, "alert:index:svc:"+svc+":open", r.ID)
    }
    _, err := pipe.Exec(ctx)
    return err
}

// åˆ†å¸ƒå¼å¹‚ç­‰ï¼šSETNX + TTL
func (c *Cache) TryMarkIdempotent(ctx context.Context, a AMAlert) (bool, error) {
    if c == nil || c.R == nil { return true, nil }
    k := "alert:idemp:" + a.Fingerprint + "|" + a.StartsAt.UTC().Format(time.RFC3339Nano)
    ok, err := c.R.SetNX(ctx, k, "1", 10*time.Minute).Result()
    return ok, err
}
```

åœ¨ handler ä¸­æ¥å…¥ï¼ˆä¼ªç ï¼‰ï¼š

```go
// å¹‚ç­‰çŸ­è·¯ï¼ˆè·¨å®ä¾‹ï¼‰
if ok, _ := h.cache.TryMarkIdempotent(c, a); !ok {
    continue
}
// DB æˆåŠŸåå†™é€š Redis
_ = h.cache.WriteIssue(c, row, a)
```

å¤±è´¥å¤„ç†ï¼šRedis å¤±è´¥ä¸å½±å“ HTTP ä¸»æµç¨‹ï¼ˆAlertmanager ä¾§é‡è¯•ä¾èµ– 2xxï¼‰ï¼Œä½†éœ€è¦æ—¥å¿—æ‰“ç‚¹ä¸å‘Šè­¦ï¼›åç»­å¯åœ¨å®šæ—¶ä»»åŠ¡åšè¡¥å¿ï¼ˆæ‰«ææœ€è¿‘ N åˆ†é’Ÿçš„ DB è®°å½•å›å¡« Redisï¼‰ã€‚

å¿«é€ŸéªŒè¯ï¼š

```bash
# è§¦å‘ä¸€æ¬¡ webhook ååœ¨ Redis æŸ¥çœ‹
redis-cli --raw keys 'alert:*'
redis-cli --raw get alert:issue:<id>
redis-cli --raw smembers alert:index:open | head -n 10
redis-cli ttl alert:issue:<id>
redis-cli --raw keys 'service_state:*'
redis-cli --raw get service_state:serviceA:v1.3.7
redis-cli --raw smembers service_state:index:health:Error
```

â¸»

â‘¨ æˆåŠŸ/å¤±è´¥è¿”å›ä¸æ—¥å¿—
	â€¢	è¿”å›ï¼šç»Ÿä¸€ 200 {"ok": true, "created": <n>}ï¼Œå³ä½¿ä¸ªåˆ«è®°å½•å¤±è´¥ä¹Ÿå¿«é€Ÿè¿”å›ï¼Œé¿å… Alertmanager é˜»å¡é‡è¯•ã€‚
	â€¢	æ—¥å¿—ï¼šæŒ‰ alertname/service/severity/fingerprint æ‰“ç‚¹ï¼›é”™è¯¯åŒ…å« SQLSTATE/å †æ ˆï¼›ç»Ÿè®¡æ¥æ”¶/è§£æ/æ’å…¥è€—æ—¶åˆ†ä½ã€‚

â¸»

â‘© æœ€å°è”è°ƒï¼ˆäººå·¥æ¨¡æ‹Ÿï¼‰

firing æ¨¡æ‹Ÿï¼š

curl -X POST http://localhost:8080/v1/integrations/alertmanager/webhook \
  -H 'Content-Type: application/json' \
  -d '{
    "receiver":"our-webhook",
    "status":"firing",
    "alerts":[
      {
        "status":"firing",
        "labels":{
            "alertname":"HighRequestLatency",
            "service":"serviceA",
            "severity":"P1",
            "idc":"yzh",
            "service_version": "v1.3.7"
            },
        "annotations":{"summary":"p95 latency over threshold","description":"apitime p95 > 450ms"},
        "startsAt":"2025-05-05T11:00:00Z",
        "endsAt":"0001-01-01T00:00:00Z",
        "generatorURL":"http://prometheus/graph?g0.expr=...",
        "fingerprint":"3b1b7f4e8f0e"
      }
    ],
    "groupLabels":{"alertname":"HighRequestLatency"},
    "commonLabels":{"service":"serviceA","severity":"P1"},
    "version":"4"
  }'

å…¥åº“åï¼Œalert_issues é‡Œåº”çœ‹åˆ°ï¼š
	â€¢	state=Open
	â€¢	alertState=Pending
	â€¢	level=P1
	â€¢	title="p95 latency over threshold"
	â€¢	label ä¸­åŒ…å« am_fingerprint/generatorURL/groupKey/...
	â€¢	alertSince=2025-05-05 11:00:00+00

åŒæ—¶ï¼Œservice_states é‡Œåº”çœ‹åˆ°/æ›´æ–°ï¼ˆæŒ‰ service+versionï¼‰ï¼š
	â€¢	service=serviceA
	â€¢	version=ï¼ˆè‹¥ labels ä¸­æœ‰ service_version åˆ™ä¸ºå…¶å€¼ï¼Œå¦åˆ™ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
	â€¢	report_at=ä¸ alert_since ä¸€è‡´ï¼ˆè‹¥å·²å­˜åœ¨åˆ™ä¿ç•™æ›´æ—©çš„ report_atï¼‰
	â€¢	health_state=Error
	â€¢	detail/resolved_at/correlation_id ä¸ºç©º

Redis ä¸­åº”çœ‹åˆ°ï¼š
	â€¢	key: alert:issue:<id> å€¼ä¸º JSON ä¸” TTLâ‰ˆ3 å¤©
	â€¢	é›†åˆ alert:index:open ä¸­åŒ…å« <id>
	â€¢	è‹¥æœ‰ service=serviceAï¼Œåˆ™ alert:index:svc:serviceA:open åŒ…å« <id>

â¸»